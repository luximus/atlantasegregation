import json
import os
import zipfile
from dataclasses import dataclass
from functools import wraps
from itertools import product
from typing import Callable, Any, Hashable

import geopandas as gpd
import pandas as pd
from tqdm import tqdm  # Progress bars

from util import approx_square_distance_on_earth


@dataclass(frozen=True)
class _Result:
    args: tuple[Any, ...]
    kwargs: dict[Hashable, Any]
    result: Any


_saved_results: dict[Callable, _Result] = {}


def save_result(func: Callable):
    """
    Decorate a function. If the decorated function is called with the same arguments as a previous call, the value of
    the previous call will be returned immediately instead of running the function again. This decorator should not be
    used on functions that might return different results even if the arguments are the same.
    """
    @wraps(func)
    def wrapper(*args, **kwargs):
        if saved_result := _saved_results.get(func, None):
            if saved_result.args == args and saved_result.kwargs == kwargs:
                return saved_result.result
        result = func(*args, **kwargs)
        _saved_results[func] = _Result(args, kwargs, result)
        return result

    return wrapper


GEO_TYPE_SEQUENCE = ['REGION', 'DIVISION', 'STATE', 'COUNTY', 'COUSUB', 'PLACE', 'TRACT', 'BLKGRP', 'BLOCK']


@save_result
def read_geo_header_record(archive_filename: str | os.PathLike,
                           filename: str,
                           fields: dict[str, tuple[int, int, type | Callable[[str], Any]]]) -> pd.DataFrame:
    """
    Read a geography header record from the U.S. 2010 Decennial Census Summary File 1 segmented file.
    :param archive_filename: Path to a ZIP file containing the geography header record.
    :param filename: The name of the geography header record file within the archive.
    :param fields: A dictionary whose keys are the data dictionary reference names and whose values are a 3-tuple. The
    first value in the tuple should be the length of the data field in characters. The second value should be the
    index of the character where the field starts, where the first index is 1. The third value should be the Python type
    of the field.
    :return: A pandas DataFrame containing the fields for each location in the geography header record.
    """
    with zipfile.ZipFile(archive_filename, mode='r') as z:
        with z.open(filename) as file:
            result = {field: [] for field in fields.keys()}
            for line in file:
                for field, instruction in fields.items():
                    length, starting_position, type_ = instruction
                    starting_position -= 1
                    result[field].append(type_(line[starting_position:starting_position + length].strip()))

    return pd.DataFrame(result)


@save_result
def get_all_of_type_in_location(df: pd.DataFrame, type_: str, name: str) -> pd.DataFrame:
    """
    Get all locations of a specific type in a location given by name.
    :param df: A pandas DataFrame generated by the `read_geo_header_record` function.
    :param type_: The type of all the sub-locations.
    :param name: The name of the super-location.
    :return: A DataFrame only containing all the locations in the location whose name was passed in.
    """
    location_row = df.loc[df['NAME'] == name].iloc[0]
    if isinstance(location_row, pd.DataFrame):
        location_row = location_row.iloc[0]
    for ty in reversed(GEO_TYPE_SEQUENCE):
        if code := location_row[ty]:
            location_code = code
            location_type = ty
            break
    else:
        raise ValueError('invalid data')

    all_in_location = df.loc[df[location_type] == location_code]
    all_as_deep_as_type_in_location = all_in_location.loc[all_in_location[type_] != '']
    if type_ == 'BLOCK':
        return all_as_deep_as_type_in_location
    else:
        return all_as_deep_as_type_in_location.loc[
            all_as_deep_as_type_in_location[GEO_TYPE_SEQUENCE[GEO_TYPE_SEQUENCE.index(type_) + 1]] == '']


@save_result
def get_all_inhabited_blocks_in_atlanta() -> pd.DataFrame:
    """
    Get all inhabited blocks in Atlanta, GA, with file linking information, the total number of housing units, and GPS
    coordinates.
    :return: A pandas DataFrame containing blocks in Atlanta, GA, with file linking information, the total number of
    housing units, and GPS coordinates.
    """
    df = read_geo_header_record(  # Read the geography header record for Georgia
        archive_filename='atlantasegregation/data/rawdata/gageo2010.zip',
        filename='gageo2010.sf1',
        fields={
            'FILEID': (6, 1, str),  # The indices come from the technical documentation for the dataset (figure 2-5)
            'STUSAB': (2, 7, str),
            'SUMLEV': (3, 9, str),
            'GEOCOMP': (2, 12, str),
            'CHARITER': (3, 14, str),
            'CIFSN': (2, 17, str),
            'LOGRECNO': (7, 19, int),
            'REGION': (1, 26, str),
            'DIVISION': (1, 27, str),
            'STATE': (2, 28, str),
            'COUNTY': (3, 30, str),
            'COUSUB': (5, 37, str),
            'PLACE': (5, 46, str),
            'TRACT': (6, 55, str),
            'BLKGRP': (1, 61, str),
            'BLOCK': (4, 62, str),
            'NAME': (90, 227, str),
            'HU100': (9, 328, int),
            'INTPTLAT': (11, 337, float),
            'INTPTLON': (12, 348, float)
        }
    )
    filtered_df = get_all_of_type_in_location(df, 'BLOCK', 'Atlanta city')  # Get all blocks in Atlanta
    filtered_df.drop(columns=['FILEID', 'STUSAB', 'SUMLEV', 'GEOCOMP', 'CHARITER', 'CIFSN', 'REGION', 'DIVISION',
                              'STATE', 'COUNTY', 'COUSUB', 'PLACE', 'TRACT', 'BLKGRP'],  # Remove unnecessary information
                     inplace=True)
    filtered_df.where(filtered_df['HU100'] > 0, inplace=True)  # Remove blocks without any housing units
    filtered_df.dropna(how='all', inplace=True)  # Drop all columns where one of the remaining values is empty
    filtered_df.reset_index(drop=True, inplace=True)  # Reset the indices of the DataFrame
    return filtered_df


@save_result
def get_block_data() -> pd.DataFrame:
    """Get all the necessary data for each census block with at least one housing unit in Atlanta in 2010."""
    housing_info_df = pd.read_csv('atlantasegregation/data/rawdata/gah3h62010.csv')
    geography_info_df = get_all_inhabited_blocks_in_atlanta()
    return geography_info_df.merge(housing_info_df, on='LOGRECNO', how='inner')


@save_result
def get_atlanta_boundary() -> gpd.GeoDataFrame:
    print('Getting Atlanta boundary...')
    boundary = gpd.read_file('atlantasegregation/data/rawdata/atlanta_city_boundary/Official_City_Boundary.shp')
    return boundary


RACES = ['White',
         'Black or African American',
         'American Indian or Alaska Native',
         'Asian',
         'Native Hawaiian or Other Pacific Islander',
         'Some Other Race',
         'Two or More Races']


@save_result
def get_initial_state() -> gpd.GeoDataFrame:
    """
    Get the initial state of the simulation (or the initial state of Atlanta as recorded in the 2010 Decennial Census).
    :return: A DataFrame containing the relevant data for the initial state of the simulation.
    """
    gdf = None
    if os.path.isfile('atlantasegregation/data/initial_state.geojson'):
        gdf = gpd.read_file('atlantasegregation/data/initial_state.geojson')
        # Convert the lists, which are currently stored as strings, to Python lists. Since the strings so happen to be
        # in JSON form, we use JSON to convert them. Suffice it to say, the data should be checked as this opens the
        # window for a denial of service.
        gdf['neighbors'] = [json.loads(neighbor) for neighbor in gdf['neighbors']]

    print('Generating initial state...')
    if gdf is None:
        print('Getting data...')
        atlanta_boundary = get_atlanta_boundary()
        df = get_block_data()
        gdf = gpd.GeoDataFrame({  # Rename all the columns to be more descriptive
            'Block': df['BLOCK'],
            'Total': df['HU100'],
            'White': df['H0060002'],
            'Black or African American': df['H0060003'],
            'American Indian or Alaska Native': df['H0060004'],
            'Asian': df['H0060005'],
            'Native Hawaiian or Other Pacific Islander': df['H0060006'],
            'Some Other Race': df['H0060007'],
            'Two or More Races': df['H0060008'],
            'Occupied': df['H0030002'],
            'Vacant': df['H0030003'],
            'geometry': gpd.points_from_xy(df['INTPTLON'], df['INTPTLAT'], crs=atlanta_boundary.crs)
        }, crs=atlanta_boundary.crs)

    # Find neighbors
    if 'neighbors' not in gdf:
        gdf['neighbors'] = [[] for _ in range(len(gdf))]
        for (index1, point1), (index2, point2) in tqdm(product(enumerate(gdf['geometry']), repeat=2),
                                                       total=len(gdf) ** 2,
                                                       desc='Finding neighbors',
                                                       bar_format='{desc}: {percentage:3.0f}%|{bar}| {elapsed}<{remaining}'):
            if index1 == index2:
                continue
            if approx_square_distance_on_earth(point1,
                                               point2) <= 0.25:  # Neighboring blocks are ones whose internal points are less than half a mile away
                gdf.iloc[index1]['neighbors'].append(index2)

    # Get the sums for each race, as well as all occupied housing units, in all neighboring blocks. This step is here
    # for efficiency.
    if any(f'{race} neighbors' not in gdf for race in RACES + ['Occupied']):
        target_races = [race for race in RACES + ['Occupied'] if f'{race} neighbors' not in gdf]
        gdf = gdf.assign(**{f'{race} neighbors': [0 for _ in range(len(gdf))] for race in target_races})
        for index, block in tqdm(gdf.iterrows(),
                                 total=len(gdf),
                                 desc='Preloading neighbor information',
                                 bar_format='{desc}: {percentage:3.0f}%|{bar}| {elapsed}<{remaining}'):
            neighbors = gdf.take(block['neighbors'], axis=0)  # Get all neighbors
            if len(neighbors) == 0:
                continue
            neighbor_sums = neighbors.take([gdf.columns.get_loc(race) for race in target_races], axis=1).sum()
            for race in target_races:
                gdf.iloc[index, gdf.columns.get_loc(f'{race} neighbors')] = neighbor_sums[race]

    return gdf


if __name__ == '__main__':
    gdf: gpd.GeoDataFrame = get_initial_state()
    print(gdf.head())
    print('Writing file...')
    with open('./atlantasegregation/data/initial_state.geojson', mode='w') as fp:
        json.dump(json.loads(gdf.to_json()), fp, indent=2)
